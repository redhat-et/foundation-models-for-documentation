{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c265b0bb-6c05-4c5d-9a33-b1d47cf4aee9",
   "metadata": {},
   "source": [
    "# Q-A Evaluating metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a29f052-5e4d-4f7b-b0eb-a2dcdfc213e5",
   "metadata": {},
   "source": [
    "Question Answering Language Models (LLMs) are typically evaluated using a variety of metrics to assess their accuracy and effectiveness in answering questions. Here are some of the common metrics used for evaluating QA LLMs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca11e164-1673-42ad-a047-6caf8a45cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from rouge import Rouge\n",
    "from jiwer import wer\n",
    "from evaluate import load\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "523869ff-ddc0-4311-818a-3db735763077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv(\"credentials.env\"), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9457b3b0-1c5b-4507-ab59-67cebb4b7748",
   "metadata": {},
   "source": [
    "**Exact Match (EM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462213fa-bf50-4e55-a7f2-3249ea3dc7cd",
   "metadata": {},
   "source": [
    "The Exact Match (EM) metric is a binary metric used to evaluate the performance of a question-answering system. It measures whether the generated answer by the LLM exactly matches the correct answer. If the generated answer is identical to the correct answer, then the EM score is 1, indicating a perfect match between the generated answer and the correct answer. Otherwise, if the generated answer is different from the correct answer, the EM score is 0, indicating that the LLM did not produce the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "615f002a-1c86-4084-b310-1d7ba7bd7056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match score: 0\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(text):\n",
    "    # Lowercase and remove non-alphanumeric characters\n",
    "    text = re.sub(r\"\\W\", \" \", text.lower())\n",
    "    # Remove leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    # Collapse multiple whitespace into a single whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "# Example usage\n",
    "prediction = \"I am going to school\"\n",
    "ground_truth = \"I am going to college\"\n",
    "em_score = exact_match_score(prediction, ground_truth)\n",
    "print(f\"Exact Match score: {em_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc8064-3260-43e6-aa96-07a97255a6b5",
   "metadata": {},
   "source": [
    "The EM metric is a simple and straightforward way to evaluate the performance of a question-answering system, and it is often used in conjunction with other metrics, such as the F1 score, to provide a more comprehensive evaluation of the system's performance. However, the EM metric has its limitations, as it is very strict and unforgiving, and it does not take into account partial matches or variations in the correct answer.\n",
    "For example, if the correct answer is \"I am going to college\", and the generated answer is \"I am going to school\", the EM score would be 0, even though the generated answer is very close to the correct answer. Therefore, the EM metric should be used in conjunction with other metrics to provide a more comprehensive evaluation of the performance of a question-answering system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c02bf38-edfa-483e-8000-a0c0a793e8a4",
   "metadata": {},
   "source": [
    "**F1-Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332427be-15b8-4b8b-a330-dafda1128ad1",
   "metadata": {},
   "source": [
    "The F1 score is a commonly used metric for evaluating the performance of question-answering systems, particularly in cases where the answer is expected to be a span of text within a larger document. The F1 score is a harmonic mean of precision and recall, which are two other commonly used metrics in natural language processing.The F1 score is the harmonic mean of precision and recall, and it provides a single metric that balances both precision and recall. The F1 score ranges from 0 to 1, with a higher score indicating better performance. An F1 score of 1 means that the LLM generated the correct answer with 100% accuracy and completeness, while a score of 0 means that the LLM did not generate any correct answers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0076b4f-da5f-42bf-87a7-9e4ffbf31450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.0\n"
     ]
    }
   ],
   "source": [
    "def f1_score_metric(prediction, ground_truth):\n",
    "    labels = list(set([prediction, ground_truth]))\n",
    "    prediction_label = labels.index(prediction)\n",
    "    ground_truth_label = labels.index(ground_truth)\n",
    "    return f1_score([ground_truth_label], [prediction_label], average='weighted')\n",
    "\n",
    "# Example usage\n",
    "prediction = \"I am going to school\"\n",
    "ground_truth = \"I am going to college\"\n",
    "f1 = f1_score_metric(prediction, ground_truth)\n",
    "print(f\"F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e810e6-be3c-4f81-a0e2-81891ea4f386",
   "metadata": {},
   "source": [
    "In question answering, the F1 score is often used in conjunction with other metrics, such as the Exact Match (EM) score, to provide a more comprehensive evaluation of the system's performance. The F1 score is particularly useful when the correct answer is expected to be a span of text within a larger document, as it takes into account the position and length of the generated answer in relation to the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02fff2e-784e-46e6-ac8f-56c8a8842b59",
   "metadata": {},
   "source": [
    "**BLEU (Bilingual Evaluation Understudy) Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a72ef49-203f-43ee-ab55-fa269cd41c31",
   "metadata": {},
   "source": [
    "BLEU (Bilingual Evaluation Understudy) Score is a metric used to evaluate the quality of machine-generated text against human-generated text. It is commonly used to evaluate the performance of machine translation systems, but it can also be used to evaluate other text generation tasks such as summarization, question answering, and text completion.\n",
    "\n",
    "The BLEU score is calculated by first computing the precision of the machine-generated text for each n-gram size from 1 to N, where N is typically 4. Precision is the number of n-grams in the machine-generated text that also appear in the human-generated text, divided by the total number of n-grams in the machine-generated text. The precision values are then combined using a geometric mean to produce the final BLEU score.\n",
    "\n",
    "One limitation of the BLEU score is that it does not take into account the fluency or coherence of the machine-generated text, only its similarity to the human-generated text in terms of n-gram overlap. For this reason, the BLEU score should be used in conjunction with other metrics such as perplexity and human evaluation to provide a more comprehensive evaluation of the quality of the machine-generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c93451ca-4233-4cb2-8586-bfcc5ac556fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.668740304976422\n"
     ]
    }
   ],
   "source": [
    "def bleu_score_metric(prediction, ground_truth):\n",
    "    reference = [ground_truth.split()]\n",
    "    candidate = prediction.split()\n",
    "    return nltk.translate.bleu_score.sentence_bleu(reference, candidate)\n",
    "\n",
    "# Example usage\n",
    "prediction = \"I am going to school\"\n",
    "ground_truth = \"I am going to college\"\n",
    "bleu = bleu_score_metric(prediction, ground_truth)\n",
    "print(f\"BLEU score: {bleu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d33443-e653-4a79-bc6f-1e6e6565558a",
   "metadata": {},
   "source": [
    "The score ranges from 0 to 1, with higher scores indicating better performance. However, it is important to note that the BLEU score has its limitations, and should be used in combination with other metrics and human evaluation to fully assess the quality and usefulness of machine-generated text. When interpreting BLEU scores, extremely low scores (e.g., on the order of 1e-154 or 1e-231) suggest that the machine-generated text is very dissimilar to the reference text, while scores closer to 1 suggest that the machine-generated text is more similar to the reference text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f8ab57-0c94-451e-8330-db8ddce1358f",
   "metadata": {},
   "source": [
    "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation) Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee6cffe-3f00-44da-a023-f5b3bc24160c",
   "metadata": {},
   "source": [
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) Score is a metric used to evaluate the quality of machine-generated text against human-generated text. It is commonly used to evaluate the performance of text summarization systems, but it can also be used to evaluate other text generation tasks such as machine translation and question answering.\n",
    "\n",
    "ROUGE score measures the overlap between the machine-generated summary and the human-generated summary using n-gram co-occurrence statistics. It is based on the recall of n-gram overlapping between machine-generated text and the reference (human-generated text). The score ranges from 0 to 1, with 1 indicating a perfect match between the two texts. The higher the ROUGE score, the better the machine-generated text is considered to be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a4e09ec-fcc6-4f2a-9c31-48e73c1228e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE score: 0.7999999950000002\n"
     ]
    }
   ],
   "source": [
    "def rouge_score_metric(prediction, ground_truth):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(prediction, ground_truth)\n",
    "    return scores[0][\"rouge-1\"][\"f\"]\n",
    "\n",
    "# Example usage\n",
    "prediction = \"I am going to school\"\n",
    "ground_truth = \"I am going to college\"\n",
    "rouge = rouge_score_metric(prediction, ground_truth)\n",
    "print(f\"ROUGE score: {rouge}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5477bd2-163f-4de6-ba8a-5d2483a01854",
   "metadata": {},
   "source": [
    "In above case, the ROUGE score is 0.7999999950000002, which is a relatively high score and suggests that there is a good amount of overlap between the model-generated text and reference text. One limitation of the ROUGE score is that it only evaluates the overlap between the machine-generated text and the reference, without considering other important factors such as fluency, coherence, and content preservation. As a result, the ROUGE score should be used in conjunction with other metrics such as BLEU score, perplexity, and human evaluation to provide a more comprehensive evaluation of the quality of the machine-generated text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ef1566-4e00-4408-9dc7-66288de504aa",
   "metadata": {},
   "source": [
    "**WER (Word Error Rate)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3923ad7-b0b9-48a5-bf9d-25f9bff43788",
   "metadata": {},
   "source": [
    "WER (Word Error Rate) is a metric used for evaluating the performance of speech recognition systems or OCR (Optical Character Recognition) systems. It measures the percentage of words in the recognized text that differ from the words in the reference text. WER is calculated by dividing the total number of word errors (insertions, deletions, and substitutions) by the total number of words in the reference text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47d07341-5bf3-4a6d-8de1-62a090f92733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER score: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prediction = \"I am going to school\"\n",
    "ground_truth = \"I am going to college\"\n",
    "wer = wer(prediction, ground_truth)\n",
    "print(f\"WER score: {wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552d8e9-5258-4923-b77a-2e6d49e0b4f8",
   "metadata": {},
   "source": [
    "A WER (Word Error Rate) score of 0.2 means that 20% of the words in the recognized text are different from the words in the reference text. In other words, there is an error rate of 20%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9478f078-5010-4a6a-b61c-646e9d45ab9e",
   "metadata": {},
   "source": [
    "## Evaluation for Generative QA model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de1eb7-46db-4c38-b324-3382ee03bf27",
   "metadata": {},
   "source": [
    "### [BERTScore](https://github.com/Tiiiger/bert_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a54ac6c-1a47-4002-a455-b52693cdf23c",
   "metadata": {},
   "source": [
    "BERTScore is a metric that evaluates the quality of generated text by comparing it to reference text. BERTScore uses the pre-trained BERT (Bidirectional Encoder Representations from Transformers) model to encode both the generated text and reference text. It then calculates the similarity between the two encodings by computing the cosine similarity between them. BERTScore has been shown to outperform other embedding-based metrics like ROUGE and BLEU in terms of correlation with human judgments of text quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cac75e4e-e700-4143-afa3-af1082da3b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': [1.0, 0.9577858448028564], 'recall': [1.0, 0.9577858448028564], 'f1': [1.0, 0.9577858448028564], 'hashcode': 'distilbert-base-uncased_L5_no-idf_version=0.3.12(hug_trans=4.27.2)'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bertscore = load(\"bertscore\")\n",
    "predictions = [\"hello world\", \"I am going to school\"]\n",
    "references = [\"hello world\", \"I am going to college\"]\n",
    "results = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c767418-f899-45de-8d37-b87c3e169ec8",
   "metadata": {},
   "source": [
    "In this case, the output shows that the precision, recall, and F1 score values for the first phrase \"hello world\" are all 1.0, indicating that the generated text is identical to the reference text. However, for the second phrase \"I am going to school\", the precision, recall, and F1 score values are all slightly lower, at around 0.96. This indicates that the generated text is similar to the reference text, but not identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0296a-9200-4a0b-853f-c2256795f77e",
   "metadata": {},
   "source": [
    "# LLM Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654898b7-21a8-451b-b048-d04803a3fd3f",
   "metadata": {},
   "source": [
    "We can evaluate generic question answering problems using [Langchain](https://langchain.readthedocs.io/en/latest/use_cases/evaluation/question_answering.html). Here is the situation where we have an example containing a question and its corresponding answer, and we want to measure how well the language model does at answering those questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fcef2a3-7138-48a2-9252-c5e852ea7ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=\"Question: {question}\\nAnswer:\", input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d12caa65-fd1c-401d-9332-ef5eaed2cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6f0cc7e-8732-4260-8b8e-b6c014a49068",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\",\n",
    "        \"answer\": \"11\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": 'Is the following sentence plausible? \"Joao Moutinho caught the screen pass in the NFC championship.\"',\n",
    "        \"answer\": \"No\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bafc0bcc-5e8c-4226-8f4a-ac792c154f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = chain.apply(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70418d09-0495-4e25-a013-e8a1e07c9a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': ' 11 tennis balls'},\n",
       " {'text': ' No, this sentence is not plausible. Joao Moutinho is a professional soccer player, not an American football player, so it is not likely that he would be catching a screen pass in the NFC championship.'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de02168-580c-4584-8859-8ff19614da2f",
   "metadata": {},
   "source": [
    "We can see that if we tried to just do exact match on the answer answers (11 and No) they would not match what the lanuage model answered. However, semantically the language model is correct in both cases. In order to account for this, we can use a language model itself to evaluate the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8d853d7-3880-438e-9e17-a91779a23c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs = eval_chain.evaluate(examples, predictions, question_key=\"question\", prediction_key=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49a089cb-681c-440a-834c-097d9892aad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "Real Answer: 11\n",
      "Predicted Answer:  11 tennis balls\n",
      "Predicted Grade:  CORRECT\n",
      "\n",
      "Example 1:\n",
      "Question: Is the following sentence plausible? \"Joao Moutinho caught the screen pass in the NFC championship.\"\n",
      "Real Answer: No\n",
      "Predicted Answer:  No, this sentence is not plausible. Joao Moutinho is a professional soccer player, not an American football player, so it is not likely that he would be catching a screen pass in the NFC championship.\n",
      "Predicted Grade:  CORRECT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, eg in enumerate(examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + eg['question'])\n",
    "    print(\"Real Answer: \" + eg['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['text'])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981784e1-0c4a-4563-99cb-b6b9fc71feb4",
   "metadata": {},
   "source": [
    "We can also customize the prompt that is used. Here is an example prompting it using  a score from 0 to 100. The custom prompt requires 3 input variables: “query”, “answer” and “result”. Where “query” is the question, “answer” is the ground truth answer, and “result” is the predicted answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c813e1fa-9760-43de-aba5-822201b9d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
    "You are grading the following question:\n",
    "{query}\n",
    "Here is the real answer:\n",
    "{answer}\n",
    "You are grading the following predicted answer:\n",
    "{result}\n",
    "What grade do you give from 0 to 10, where 0 is the lowest (very low similarity) and 100 is the highest (very high similarity)?\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01e0e497-2d1d-40f2-b58e-977479b8ab18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '\\n100. The predicted answer is exactly the same as the real answer, so it deserves a perfect score.'},\n",
       " {'text': '\\n90. The predicted answer is very accurate and shows a good understanding of the context.'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "evalchain = QAEvalChain.from_llm(llm=llm,prompt=PROMPT)\n",
    "evalchain.evaluate(examples, predictions, question_key=\"question\", answer_key=\"answer\", prediction_key=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401b3cba-f0fa-4a54-a817-644c6cf43351",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774838a-359c-46f4-8543-0d8bc765e74b",
   "metadata": {},
   "source": [
    "In conclusion, evaluating the performance of Language Model (LM) question answering systems is essential to measure their accuracy and effectiveness. Different evaluation metrics such as Exact Match, F1 score, BLEU score, ROUGE score, and WER are commonly used to measure the quality of LM responses. Additionally, evaluation metrics for generative models such as BERT_score and LLM grading can provide valuable insights into the quality and effectiveness of the language models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
