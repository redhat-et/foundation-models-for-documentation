{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f4bcbac-1192-482f-ad65-212922e10a4c",
   "metadata": {},
   "source": [
    "# Langchain Evaluation (Criteria based evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e425fb-5bcf-4740-a25d-9eef1baef216",
   "metadata": {},
   "source": [
    "Reference: https://python.langchain.com/docs/guides/evaluation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adde0b42-58ae-448e-80fd-ab7a06d93ca5",
   "metadata": {},
   "source": [
    "When aiming to shape our model's output according to personalized criteria, the use of criteria-based evaluation becomes essential. This notebook will delve into the exploration of this approach, providing insights into how custom criteria can be effectively employed in evaluating model outputs.\n",
    "\n",
    "In this notebook, we are taking OpenAI api key to judge the model output based on cretain critera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d509499-bf55-4ecc-a42f-85f4aef8832f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv(\"credentials.env\"), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9641b526-62a4-4a72-abd1-bd44a3cef8b6",
   "metadata": {},
   "source": [
    "Lets load the packages first,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "245b473d-75bb-4539-83d0-11bfcb9b50a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e614d4-1a85-4d7c-a11d-64d56f7e068b",
   "metadata": {},
   "source": [
    "To find the list of pre-implemented criteria,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f687594-3739-43b6-abc0-8402effe42fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Criteria.CONCISENESS: 'conciseness'>,\n",
       " <Criteria.RELEVANCE: 'relevance'>,\n",
       " <Criteria.CORRECTNESS: 'correctness'>,\n",
       " <Criteria.COHERENCE: 'coherence'>,\n",
       " <Criteria.HARMFULNESS: 'harmfulness'>,\n",
       " <Criteria.MALICIOUSNESS: 'maliciousness'>,\n",
       " <Criteria.HELPFULNESS: 'helpfulness'>,\n",
       " <Criteria.CONTROVERSIALITY: 'controversiality'>,\n",
       " <Criteria.MISOGYNY: 'misogyny'>,\n",
       " <Criteria.CRIMINALITY: 'criminality'>,\n",
       " <Criteria.INSENSITIVITY: 'insensitivity'>,\n",
       " <Criteria.DEPTH: 'depth'>,\n",
       " <Criteria.CREATIVITY: 'creativity'>,\n",
       " <Criteria.DETAIL: 'detail'>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import Criteria\n",
    "\n",
    "list(Criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e78f5-3cbd-4ddf-b34c-9e2f297d3701",
   "metadata": {},
   "source": [
    "The list mentioned above outlines the different criteria used to assess model responses. Notably, when it comes to \"correctness,\" having an established correct answer is essential for evaluation. However, for other criteria, the model's response on its own is adequate for assessment. This approach ensures a comprehensive evaluation process that considers various aspects of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15ccb2-08d0-427f-a5fc-6b3e5eff536d",
   "metadata": {},
   "source": [
    "Next, In this example, you will use the CriteriaEvalChain to check whether an output is based on criteria, 'conciseness'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1c0a284-08b6-4e5e-afb5-73c148e3e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"conciseness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e9850e9-5623-404d-badb-bec068c88e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The criterion is about the conciseness of the submission. The submission is indeed concise and to the point, as it is a single number. However, the criterion does not address the correctness of the answer. The submission is incorrect as the answer to 5+5 is 10, not 8. But since the criterion only asks for conciseness and not correctness, the submission technically meets the given criterion.\\n\\nY', 'value': 'Y', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "  prediction=\"8\",\n",
    "  input=\"5+5?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c35dc-75a3-4238-93c4-08773990048a",
   "metadata": {},
   "source": [
    "Now we do see that, although the response is incorrect. But cince our evaluation criteria is based on cconciseness. The score is 1 and value is Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384db047-cc8e-401b-82b6-9197367fd6de",
   "metadata": {},
   "source": [
    "Lets check another example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8503fbe6-45b8-4270-862e-fbc977ad8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"helpfulness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77583244-95f2-4cac-80f8-4b310c7c263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The criterion is \"helpfulness: Is the submission helpful, insightful, and appropriate?\"\\n\\nThe input is a simple arithmetic problem: 5+5?\\n\\nThe submitted answer is 8.\\n\\nThe correct answer to the input problem is 10, not 8. Therefore, the submitted answer is incorrect.\\n\\nGiven that the submitted answer is incorrect, it is not helpful or appropriate for the given task. It does not provide the correct information or insight into the problem.\\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "  prediction=\"8\",\n",
    "  input=\"5+5?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1aa47b-8a88-487c-add9-ea7aedef95a5",
   "metadata": {},
   "source": [
    "Here, with the same example output. By changing the criteria of evaluation. We see that the answer is not helpful to us since it is incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c736e5ab-fbc3-4b43-a2d3-9f08c189b2e7",
   "metadata": {},
   "source": [
    "In this way we can judge any response based on any criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4428ae8-e7ef-491d-8b9b-73e4315ab32d",
   "metadata": {},
   "source": [
    "## Defining custom criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7b311-db99-43a0-9ada-ac65e8b80f2b",
   "metadata": {},
   "source": [
    "We can also define our own criteria, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "622a2917-c091-442c-88ff-6ee927f2b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_criterion = {\n",
    "    \"numeric\": \"Does the output contain numeric or mathematical information?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8abad3a-fd28-47fa-9a2c-1486085ccb5e",
   "metadata": {},
   "source": [
    "Here we define a numeric criteria based on if an output contains any numeric or mathematical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73e82b6e-058d-4a22-aac3-8ffc355ea1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chain = load_evaluator(\n",
    "    EvaluatorType.CRITERIA,\n",
    "    criteria=custom_criterion,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c740fc6-c48d-4046-9260-acd678907944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The criterion is asking if the output contains numeric or mathematical information. The submission is \"8\", which is a numeric value. Therefore, the submission does meet the criterion, even though the answer to the mathematical problem is incorrect. \\n\\nY', 'value': 'Y', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "eval_result = eval_chain.evaluate_strings(\n",
    "  prediction=\"8\",\n",
    "  input=\"5+5?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d29709-a55a-4a37-b90e-2a46e1f26978",
   "metadata": {},
   "source": [
    "We can also define multiple criteria,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c73dee86-231f-49e7-b171-bcc4698cde1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_criteria = {\n",
    "    \"numeric\": \"Does the output contain numeric information?\",\n",
    "    \"mathematical\": \"Does the output contain mathematical information?\",\n",
    "    \"grammatical\": \"Is the output grammatically correct?\",\n",
    "    \"logical\": \"Is the output logical?\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "568d25c9-fdc6-4bec-8411-71f673f4850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chain = load_evaluator(\n",
    "    EvaluatorType.CRITERIA,\n",
    "    criteria=custom_criteria,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "185dd1e1-1cb2-4288-8412-41305b661b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'Let\\'s assess the submission based on the given criteria:\\n\\n1. Numeric: The output does contain numeric information. The answer provided is a number, \"8\". So, it meets this criterion.\\n\\n2. Mathematical: The output does contain mathematical information. The answer provided is a result of a mathematical operation, even though it\\'s incorrect. So, it meets this criterion.\\n\\n3. Grammatical: The output is grammatically correct. It\\'s a single number without any additional words or symbols that could make it grammatically incorrect. So, it meets this criterion.\\n\\n4. Logical: The output is not logical. The sum of 5 and 5 is 10, not 8. So, it does not meet this criterion.\\n\\nThe submission meets the first three criteria but fails to meet the last one.\\n\\nN', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "eval_result = eval_chain.evaluate_strings(\n",
    "  prediction=\"8\",\n",
    "  input=\"5+5?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5484b-a9c8-4a12-a870-40dd6ad28229",
   "metadata": {},
   "source": [
    "The result was judged based on multiple criterias. as shown in the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff910a87-e0bf-4d3e-a5dd-e661ce9e9747",
   "metadata": {},
   "source": [
    "Now lets evaluate the correctness criteria, where we should supply the reference text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bb3cbed-1652-419a-8c28-9edb72aa4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chain = load_evaluator(\n",
    "    \"labeled_criteria\",\n",
    "    criteria='correctness',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5dfd6128-9104-4b3b-8eb1-bc8deba421a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = eval_chain.evaluate_strings(\n",
    "    prediction = \"Moon is not the satellite of the earth\",\n",
    "    reference= \"Moon is the satellite of the earth\",\n",
    "    input = \"Name the satellite of the earth?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06a321ab-28ff-438e-bea2-544651190fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The criterion for this task is the correctness of the submission. The submission should be correct, accurate, and factual.\\n\\nThe input asks for the name of the satellite of the earth. The reference answer to this input is \"Moon is the satellite of the earth\", which is a correct and factual statement.\\n\\nThe submitted answer is \"Moon is not the satellite of the earth\". This statement is incorrect and not factual. The moon is indeed the satellite of the earth.\\n\\nTherefore, the submission does not meet the criterion of correctness.\\n\\nN', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0995a941-60e9-47f6-9e16-bc0b0528d21f",
   "metadata": {},
   "source": [
    "In above examples, we saw how can we use pre-implemented criterias or custom criteria in order to evaluate a model output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
