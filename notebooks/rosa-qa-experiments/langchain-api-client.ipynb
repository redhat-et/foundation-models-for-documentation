{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with LangChain and a custom LLM accessible via a custom API\n",
    "\n",
    "The goal of this notebook is to illustrate how to run a [LangChain](https://github.com/hwchase17/langchain) question answering chain that relies on a custom Language Model that is exposed via an API.\n",
    "\n",
    "Out of the box LangChain comes with built-in integrations for [multiple Large Language Models](https://python.langchain.com/en/latest/modules/models/llms/integrations.html). However, here we want to cover the case where we have a custom API exposed via our own custom API that does not match any of the pre-existing APIs.\n",
    "\n",
    "In particular, this example notebook interacts with a model that is hosted and exposed via the API from the [text-generation-webui](https://github.com/oobabooga/text-generation-webui/) application.\n",
    "\n",
    "References: \n",
    "- https://blog.langchain.dev/tutorial-chatgpt-over-your-data/ explains the overall workflow that this notebook follows. The example in the blog post uses OpenAI LLMs and a FAISS vector store; here we will use a locally hosted LM and ChromaDB instead.\n",
    "- https://github.com/hwchase17/langchain/blob/master/docs/modules/models/llms/examples/custom_llm.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from typing import Optional, List, Mapping, Any"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom LLM\n",
    "\n",
    "To allow LangChain to interact with our model via its custom API, we define a custom LLM wrapper around that API. Essentially, the class that we define must inherit from LangChain's `LLM` base class and provide a `_call` method that takes care of querying our model through its API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ApiLLM(LLM):\n",
    "    \n",
    "    api_url: str\n",
    "    \"The URL of the API endpoint.\"\n",
    "\n",
    "    api_params: dict\n",
    "    \"A dictionary of parameters to pass to the API.\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Call the LLM.\"\"\"\n",
    "        request = {\"prompt\": prompt, **self.api_params}\n",
    "        response = requests.post(self.api_url, json=request).json()\n",
    "        return response[\"results\"][0]['text']\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"api_url\": self.api_url, \"api_params\": self.api_params}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating our custom LLM\n",
    "\n",
    "Here we create an instance of the custom language model by providing an enpdoint URL and a set of default parameters to pass to the API calls along each prompt.\n",
    "\n",
    "Note that there is no way to pick a specific model in the API we are using here: the model will be whatever is being served by the API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API URL\n",
    "# The endpoint for the API can be stored in a .env file (see credentials_example.env)\n",
    "load_dotenv(find_dotenv(\"credentials.env\"), override=True)\n",
    "endpoint = os.environ.get(\"LLM_API_URL\", None)\n",
    "\n",
    "# Alternatively, set it manually by uncommenting the following line and replacing the URL\n",
    "#endpoint = \"https://text-generation-api-llm.example.com/api/v1/generate\"\n",
    "\n",
    "# Generation parameters\n",
    "# Reference: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
    "params = {\n",
    "    'max_new_tokens': 250,\n",
    "    'do_sample': True,\n",
    "    'temperature': 0.1,\n",
    "    'top_p': 0.1,\n",
    "    'typical_p': 1,\n",
    "    'repetition_penalty': 1.1,\n",
    "    'encoder_repetition_penalty': 1.0,\n",
    "    'top_k': 30,\n",
    "    'min_length': 0,\n",
    "    'no_repeat_ngram_size': 0,\n",
    "    'num_beams': 1,\n",
    "    'penalty_alpha': 0,\n",
    "    'length_penalty': 1,\n",
    "    'early_stopping': False,\n",
    "    'seed': -1,\n",
    "    'add_bos_token': True,\n",
    "    'truncation_length': 2048,\n",
    "    'ban_eos_token': False,\n",
    "    'skip_special_tokens': True,\n",
    "    'stopping_strings': []\n",
    "}\n",
    "\n",
    "llm = ApiLLM(api_url=endpoint, api_params=params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a sample test call to our custom LLM to verify that things work as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of the new blog.\n",
      "I’m not sure how to use this yet, but I will figure it out soon enough! This should be fun and interesting for me as well…and hopefully you too!!\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"Once upon a time,\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Document indexing\n",
    "\n",
    "This part of the notebook takes care of preparing an embeddings database for the documents that will become part of the context prompt provided our model to help it answer the question.\n",
    "\n",
    "First we load the data, which we have as a set of Markdown files, and split it in chunks that attempt to respect the logical structure determined by Markdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 documents were loaded in 7058 chunks\n"
     ]
    }
   ],
   "source": [
    "# Note: using TextLoader here instead of UnstructuredMarkdownLoader. MarkdownTextSplitter will do the job of parsing markdown\n",
    "loader = DirectoryLoader('../data/external', glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "documents = loader.load()  # FYI / note to self: with the dataset at the time of this writing, documents[42] is the FAQ\n",
    "\n",
    "# Split the documents into chunks, respecting Markdown structure.\n",
    "# The chunk size represents a trade-off between the LLM context size and the quantity of information that can be provided as context.\n",
    "text_splitter = MarkdownTextSplitter(chunk_overlap=0, chunk_size=750)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"{len(documents)} documents were loaded in {len(texts)} chunks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create (or load, if it already exists) a vector store from the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "loaded in 7058 embeddings\n",
      "loaded in 1 collections\n",
      "collection with name langchain already exists, returning existing collection\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# https://langchain.readthedocs.io/en/latest/modules/indexes/vectorstore_examples/chroma.html#persist-the-database\n",
    "db_dir = \"../data/interim\"\n",
    "docsearch = None\n",
    "if os.path.isdir(os.path.join(db_dir, \"index\")):\n",
    "    # Load the existing vector store\n",
    "    docsearch = Chroma(persist_directory=db_dir, embedding_function=embeddings)\n",
    "else:\n",
    "    # Create a new vector store\n",
    "    docsearch = Chroma.from_documents(texts, embeddings, persist_directory=db_dir)\n",
    "    docsearch.persist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering\n",
    "\n",
    "For this example we use a manually defined `RetrievalQA` chain with a custom template prompt.\n",
    "\n",
    "References:\n",
    "- https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html\n",
    "- see other notebooks in this repo for different chain approaches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"When learning about Red Hat OpenShift Service on AWS (ROSA), and considering the following context:\n",
    "========= snippets start here =========\n",
    "{context}\n",
    "========= spippets end here =========\n",
    "Given this question: {question}\n",
    "The answer to the question is:\"\"\"\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
    "chain_type_kwargs = {\"prompt\": qa_prompt}\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_kwargs={\"k\": 3}),  # k is the number of documents to retrieve\n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question answering\n",
    "\n",
    "We define a list of questions to ask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = ['Where can I see a roadmap or make feature requests for the service?',\n",
    "           'How is the pricing of Red Hat OpenShift Service on AWS calculated?',\n",
    "           'Is there an upfront commitment?',\n",
    "           'How can I delete ROSA cluster?',\n",
    "           'Can I shut down my VMs temporarily?', # https://docs.openshift.com/rosa/rosa_architecture/rosa_policy_service_definition/rosa-service-definition.html#rosa-sdpolicy-instance-types_rosa-service-definition\n",
    "           'How can I automatically deploy ROSA cluster?',\n",
    "           'How can my ROSA cluster autoscale?',\n",
    "           'How can I install aws load balancer controller',\n",
    "           'How can I install Prometheus Operator with my ROSA cluster?',\n",
    "           'What time is it?', # adversarial example\n",
    "           'How can I federate metrics to a centralized Prometheus Cluster?',\n",
    "           'What is the meaning of life?'] # adversarial example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate over the list of questions and call the QA chain for each, showing the resulting answer.\n",
    "\n",
    "**NOTE**: the results below come from an API that is exposing the [MLP-7B Base](https://www.mosaicml.com/blog/mpt-7b) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Question: Where can I see a roadmap or make feature requests for the service?\n",
      "Answer:   https://access.redhat.com/documentation/en-US/OpenShift_Service_on_AWS/1.0/html/GettingStartedGuide/#gettingstartedguide-roadmapprojectfeaturerequests\n",
      "================================================================================\n",
      "Question: How is the pricing of Red Hat OpenShift Service on AWS calculated?\n",
      "Answer:   _There's no simple way to calculate it because each customer will use different amounts of compute power over time_. For example, if your application requires more CPU than usual during peak times or when new features go live then that could increase your bill significantly.<br><br>\n",
      "For most customers we recommend using our Reserved Instance program as described below so they can lock down their price at a discount compared with On Demand prices. This allows them to budget accurately based upon what they expect to spend rather than having unpredictable bills from month to month due to usage spikes. <br><br>\n",
      "\n",
      "*Reserved instance discounts apply only to clusters created after March 31st 2020.*\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question: Is there an upfront commitment?\n",
      "Answer:   No - There are two options when it comes to paying for usage in Amazon Web Services; On Demand Pricing where customers only get billed after they launch their resources OR Reserved Instances(RI). The latter option allows users to reserve capacity ahead of time at discounted rates compared to what would otherwise cost if launched via OnDemand prices. This means that by purchasing RIs one gets access to lower costs than those offered through regular \"On demand\" instances without having any long term commitments associated with them! For more information please visit https://aws.amazon.com/ec2/pricing/.\n",
      "================================================================================\n",
      "Question: How can I delete ROSA cluster?\n",
      "Answer:   To remove an existing Kubernetes cluster from Amazon EKS, use the [eksctl delete][delete] subcommand as follows:\n",
      "\n",
      "```terminal\n",
      "# Delete the kubernetes cluster named \"my-k8s\"\n",
      "$ eksctl delete cluster my-k8s -f ~/.kube/config # Use default config file if not specified\n",
      "```\n",
      "\n",
      "[delete]: https://github.com/weaveworks/eksctl#delete\n",
      "================================================================================\n",
      "Question: Can I shut down my VMs temporarily?\n",
      "Answer:   No! The only way to stop your VM instances in Amazon EC2 is via the Console or APIs provided directly with each instance type. If you need help stopping an individual server please contact us so we may assist further.\n",
      "================================================================================\n",
      "Question: How can I automatically deploy ROSA cluster?\n",
      "Answer:   Use our automated deployment scripts!\n",
      "================================================================================\n",
      "Question: How can my ROSA cluster autoscale?\n",
      "Answer:   You need to create MachinePool definitions with minReplicas & maxReplicas defined as well as enabling auto scaling by setting EnableAutoscaling = true; then click Apply button after editing each one individually via GUI interface OR if doing it programmatically through API calls, pass those parameters along while creating new MP's\n",
      "================================================================================\n",
      "Question: How can I install aws load balancer controller\n",
      "Answer:   You need a VPC, an EKS cluster with ALBs enabled in it's configuration, you also have access keys for your account that are configured as environment variables ```export AwsAccessKeyId=\"<your key>\" export SecretsAccessKey=\"<your secret>\"` then run these commands from within the directory where they were downloaded into using bash or zsh shell scripts respectively;\n",
      "\n",
      "```shell script\n",
      "#!/bin/zsh\n",
      "source./scripts/setupEnvVariables.sh # This will set up all of our env vars needed by other files below!\n",
      "cd /path/to/albControllerDir && make deployClusterAndInstallHelmChart  && cd..\n",
      "echo 'Done!'\n",
      "exit 0\n",
      "```\n",
      "================================================================================\n",
      "Question: How can I install Prometheus Operator with my ROSA cluster?\n",
      "Answer:   Use rosa cli command line tooling or API calls from within Kubernetes itself!\n",
      "================================================================================\n",
      "Question: What time is it?\n",
      "Answer:   It's 4 o'clock in the afternoon, or 16 hours after 12 noon.\n",
      "================================================================================\n",
      "Question: How can I federate metrics to a centralized Prometheus Cluster?\n",
      "Answer:   Use an agent that supports remote write like [Prometheus Remote Write](https://prometheus.io/docs/instrumenting/exposition_formats/#remotewrite). In our case we'll be deploying one of these agents called [`mobb`](https://github.com/openshift/mobb#readme) which has been developed by RedHat specifically for Kubernetes environments but also works great when deployed outside of kubernetes too! This means no need for additional infrastructure such as sidecars etc., just install mobb once per node group within each zone / availability domain if needed depending upon how many nodes there should exist under them respectively; however keep mind they must share common network topology between themselves otherwise communication won't work properly due to lack accessability via DNS resolution mechanism provided internally only among pods residing within single host machine instance itself without external connectivity whatsoever required externally available publicly accessible internet addressable public facing IP addresses assigned globally routable IPv4 space reserved exclusively dedicated purposefully allocated solely intended purposes related directly associated closely tied tightly coupled together intimately bound close relationship strongly connected deeply intertwined intricately linked interwoven mutually dependent interconnectedly involved joined attached conjoined united combined jointly allied partnered coordinated working hand in glove harmon\n",
      "================================================================================\n",
      "Question: What is the meaning of life?\n",
      "Answer:   42.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for query in queries:\n",
    "    answers.append(qa_chain(query))\n",
    "\n",
    "# Print the answers\n",
    "for result in answers:\n",
    "    print(\"=\"*80)\n",
    "    print(\"Question:\", result[\"query\"])\n",
    "    print(\"Answer: \", result[\"result\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "This notebook presents a simple way to wrap a custom Large Language Model (LLM) that is being exposed via an API so that it can be integrated into LangChain. The integration is illustrated with a sample set of question answering for Red Hat OpenShift Service on AWS (ROSA).\n",
    "\n",
    "The integration provided here is very simple and uses a REST API. This works well enough for the use case of \"batch\" (or essentially non-interactive) question answering.\n",
    "\n",
    "For more interactive use cases (interactive question answering or conversations), the simple wrapper class could be adapted to use the streaming WebSocket API that is also available. This should provide a better user experience in this type of use case. Some sample code to access the WebSocket API in `text-generation-webui` is available in [their repository](https://github.com/oobabooga/text-generation-webui/blob/main/api-example-stream.py).\n",
    "\n",
    "There is room for improvement in the question answering process: it should be possible to improve the results by working on better prompts, trying different models, and improving the document retrieval process (quantity and quality of the data, as well as the document retrieval process). However, this is beyond the scope of this notebook, whose main purpose is to illustrate the integration of a custom API-based LLM into LangChain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "dfef540ac477f3be91e5308acd671ac2a81f2c8cc1125947821b3502f71b441e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
