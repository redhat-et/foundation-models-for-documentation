{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46992c86-b56c-446c-a86e-3b8ba9b9f383",
   "metadata": {},
   "source": [
    "# Langchain with OpenAI\n",
    "This notebook explores the [langchain framework](https://langchain.readthedocs.io/en/latest/index.html) that helps develop with large language models for various tasks including [question answering](https://langchain.readthedocs.io/en/latest/modules/indexes/chain_examples/question_answering.html). Here, we are using the open-ai api as the choice of llm. You will need to provide the api key as the `OPENAI_API_KEY` environment variable to execute this notebook. For the dataset, we use public Red Hat OpenShift on AWS (ROSA) docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37f873-5059-4272-9ee0-9e6368bc0bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain openai chromadb unstructured evaluate bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0cc37b2-3872-4357-90ff-432e665d2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756b5eb5-05dc-4343-a60a-815eccaa2826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv(\"credentials.env\"), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d063117-fd02-4ff6-99ed-085ce2f2be26",
   "metadata": {},
   "source": [
    "## Semantic search within the documents\n",
    "This step can be seen as retriever for the QA model. This performs a similar function as the dense passage retrieval and term frequency based information retrieval tasks. If we have a large collection of documents, this step filters the relevant parts for the text generators to use as input. Here, we are using [chromadb](https://www.trychroma.com/) as our choice of vectorstore following the [langchain QA tutorial](https://langchain.readthedocs.io/en/latest/modules/indexes/chain_examples/question_answering.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea318df-d9db-4245-8f78-bba45bd262e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader('../data/external/rosaworkshop/14-faq.md')\n",
    "documents = loader.load()\n",
    "text_splitter = MarkdownTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f54bcf-f985-4e76-b3ac-56f8c0ec8b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      "\n",
      "### What is Red Hat OpenShift Service on AWS (ROSA)?\n",
      "Red Hat Openshift Service on AWS (ROSA) is a fully-managed turnkey application platform that allows you to focus on what matters most, delivering value to your customers by building and deploying applications. Red Hat SRE experts manage the underlying platform so you don’t have to worry about the complexity of infrastructure management.\n",
      "\n",
      "### Where can I go to get more information/details?\n",
      "- [ROSA Webpage](https://www.openshift.com/products/amazon-openshift)\n",
      "- [ROSA Workshop](https://www.rosaworkshop.io)\n",
      "- [ROSA Documentation](https://docs.openshift.com/rosa/welcome/index.html)\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Red Hat OpenShift Service on AWS (ROSA)?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea80b8-6914-4c60-bea8-3fe572ef04c8",
   "metadata": {},
   "source": [
    "## Question answering \n",
    "\n",
    "For question answering, we use the llm chain feature of the langchain framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "351500a4-e2b9-4ee5-ba1e-35fcf6a2c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "094185b4-f88f-401b-aeae-be8a4bad09a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Red Hat OpenShift Service on AWS (ROSA) is a fully-managed turnkey application platform that allows you to focus on what matters most, delivering value to your customers by building and deploying applications. Red Hat SRE experts manage the underlying platform so you don’t have to worry about the complexity of infrastructure management.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Red Hat OpenShift Service on AWS (ROSA)?\"\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d39aae6-e762-4c4b-881b-e73fd3485710",
   "metadata": {},
   "source": [
    "## Question answering outputs with sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54e58e2c-db3b-410e-ba1c-2d4badb4e19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': ' Red Hat OpenShift Service on AWS (ROSA) is a fully-managed turnkey application platform that allows you to focus on what matters most, delivering value to your customers by building and deploying applications. It includes container management, automation (Operators), networking, load balancing, service mesh, CI/CD, firewall, monitoring, registry, authentication, and authorization capabilities. The underlying node OS used is Red Hat Enterprise Linux CoreOS (RHCOS).\\nSOURCES: ../data/external/rosaworkshop/14-faq.md'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed950b1-cfb5-46ed-98fa-984081344254",
   "metadata": {},
   "source": [
    "## Loading all the ROSA documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "555178f7-0ebd-49ef-b7c4-cb8b1049dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('../data/external', glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01d4516c-b7e8-44d2-9bcf-c24e1b6fc102",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To do: better split of docs\n",
    "text_splitter = MarkdownTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "019cac62-823a-49a5-82a7-b4273d0051b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "No existing DB found in ../data/interim, skipping load\n",
      "No existing DB found in ../data/interim, skipping load\n",
      "Exiting: Cleaning up .chroma directory\n",
      "Persisting DB to disk, putting it in the save folder ../data/interim\n"
     ]
    }
   ],
   "source": [
    "docsearch = Chroma.from_documents(texts, embeddings, persist_directory='../data/interim')\n",
    "#docsearch.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20104763-9d31-4b7d-93d1-47b5dfcf225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc178a-dcd8-4937-b6f6-8ea83e0c5be4",
   "metadata": {},
   "source": [
    "## Next, we are going to answer example questions and look at the model responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c838cb2-58d9-4017-90b9-0822ac8e2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(query, index, chain):\n",
    "    \"\"\"\n",
    "    Takes in query, index to search from, and llm chain to generate answer\n",
    "    \"\"\"\n",
    "    ## Retrieve docs\n",
    "    docs = index.similarity_search(query)\n",
    "    ## Generate answer\n",
    "    answer = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "    return answer['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f99fb43b-7343-4dbe-a527-05a13bcfcbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Where can I see a roadmap or make feature requests for the service? \n",
      " Answer:  You can submit a Jira issue for the most relevant documentation component to make feature requests for the service.\n",
      "SOURCES: ../data/external/rosa-docs/service_mesh.md\n",
      "Query: How is the pricing of Red Hat OpenShift Service on AWS calculated? \n",
      " Answer:  Red Hat OpenShift Service on AWS has three components to its cost: an hourly cluster fee, pricing per worker node, and underlying AWS infrastructure costs. The hourly cluster fee is $0.03/cluster/hour, and the pricing per worker node is $0.171 per 4vCPU/hour for on-demand consumption, $0.114 per 4vCPU/hour for a 1-year RI commitment, and $0.076 per 4vCPU/hour for a 3-year RI commitment.\n",
      "SOURCES: ../data/external/rosaworkshop/14-faq.md\n",
      "Query: Is there an upfront commitment? \n",
      " Answer:  There is no required upfront commitment.\n",
      "SOURCES: ../data/external/rosaworkshop/14-faq.md\n",
      "Query: How can I delete ROSA cluster? \n",
      " Answer:  To delete a ROSA cluster, use the ROSA CLI (`rosa`) command and delete the STS resources by using the AWS Identity and Access Management (IAM) Console.\n",
      "SOURCES: \n",
      "../data/external/rosa-docs/rosa_getting_started.md\n",
      "../data/external/rosa-docs/rosa_install_access_delete_clusters.md\n",
      "../data/external/rh-mobb/kms-_index.md\n",
      "../data/external/rh-mobb/sts-with-private-link-_index.md\n",
      "Query: How can I automatically deploy ROSA cluster? \n",
      " Answer:  To automatically deploy a ROSA cluster, use the ROSA CLI in automatic mode.\n",
      "SOURCES: ../data/external/rosaworkshop/2-deploy.md, ../data/external/rh-mobb/security-ra-index.md, ../data/external/rh-mobb/waf-alb.md\n",
      "Query: How can my ROSA cluster autoscale? \n",
      " Answer:  ROSA clusters can be autoscaled by enabling autoscaling in the ROSA CLI when deploying the cluster.\n",
      "SOURCES: ../data/external/rosaworkshop/2-deploy.md, ../data/external/rosaworkshop/14-faq.md\n",
      "Query: How can I install aws load balancer controller \n",
      " Answer:  To install the aws load balancer controller, add the helm repo and install the controller using the command provided in the source.\n",
      "SOURCES: ../data/external/rh-mobb/waf-alb.md\n",
      "Query: How can I install Prometheus Operator with my ROSA cluster? \n",
      " Answer:  To install Prometheus Operator with a ROSA cluster, you need to create a OperatorGroup and Subscription for the Prometheus Operator, install the mobb/rosa-federated-prometheus Helm Chart, and deploy the ROSA cluster.\n",
      "SOURCES:\n",
      "../data/external/rh-mobb/federated-metrics-prometheus-_index.md\n",
      "../data/external/rh-mobb/custom-alertmanager-4.9-_index.md\n",
      "../data/external/rh-mobb/sts-with-private-link-_index.md\n",
      "Query: What time is it? \n",
      " Answer:  I don't know.\n",
      "SOURCES: N/A\n",
      "Query: How can I federate metrics to a centralized Prometheus Cluster? \n",
      " Answer:  To federate metrics to a centralized Prometheus Cluster, you can deploy a Prometheus instance and configure it to send alerts to themselves, and use Prometheus' federated metrics feature and the Prometheus Operator.\n",
      "SOURCES: \n",
      "Tommer Amber's guide: https://medium.com/@tamber/2-mini-how-to-guides-for-prometheus-on-openshift-federation-custom-infrastructure-alerting-8ec70061405d\n",
      "Query: What is the meaning of life? \n",
      " Answer:  I don't know.\n",
      "SOURCES: N/A\n"
     ]
    }
   ],
   "source": [
    "queries = ['Where can I see a roadmap or make feature requests for the service?',\n",
    "           'How is the pricing of Red Hat OpenShift Service on AWS calculated?',\n",
    "           'Is there an upfront commitment?',\n",
    "           'How can I delete ROSA cluster?',\n",
    "           'How can I automatically deploy ROSA cluster?',\n",
    "           'How can my ROSA cluster autoscale?',\n",
    "           'How can I install aws load balancer controller',\n",
    "           'How can I install Prometheus Operator with my ROSA cluster?',\n",
    "           'What time is it?',\n",
    "           'How can I federate metrics to a centralized Prometheus Cluster?',\n",
    "           'What is the meaning of life?']\n",
    "\n",
    "for query in queries:\n",
    "    answer = answer_question(query, docsearch, chain)\n",
    "    print(f'Query: {query} \\n Answer: {answer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8148e-bb4f-448d-b31b-d389c8a0390b",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "* Understand and experiment with different types of [chains](https://langchain.readthedocs.io/en/latest/modules/chains/combine_docs.html) and update the `chain_type` parameter.\n",
    "* Explore Hyde and other embedding sources.\n",
    "* Explore Bloom and custom llm with langchain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
