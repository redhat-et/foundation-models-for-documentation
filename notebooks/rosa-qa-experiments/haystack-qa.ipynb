{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d0b16b-7692-4275-9c6a-41af7d2be02e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simple Question Answering (QA) with Haystack\n",
    "\n",
    "The purpose of this notebook is to explore how [Haystack](https://haystack.deepset.ai) can be used as a framework for Question Answering (QA) systems.\n",
    "\n",
    "A few different approaches to QA are tested below:\n",
    "\n",
    "- [Extractive QA](#Extractive-QA)\n",
    "- [Generative QA](#Generative-QA), using:\n",
    "  - [Long-form Question Answering](#LFQA) using sequence-to-sequence (`Seq2Seq`) models\n",
    "  - [OpenAI completions API](#OpenAI), i.e. GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72fa015-52b1-4a44-a64e-6c012f3a2112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is a set of questions we want to ask and see which answers we obtain from the models\n",
    "\n",
    "questions = [\n",
    "    \"Do I have to sign a contract with Red Hat in order to deploy a ROSA cluster?\",\n",
    "    \"Is ROSA GDPR Compliant?\",             # https://www.rosaworkshop.io/rosa/14-faq/#is-rosa-gdpr-compliant\n",
    "    \"How can I upgrade my ROSA cluster?\",  # https://www.rosaworkshop.io/rosa/9-upgrade/\n",
    "    \"What is STS?\",      # https://www.rosaworkshop.io/rosa/15-sts_explained/#what-is-aws-sts-security-token-service\n",
    "    \"How is ROSA related to Kubernetes?\", # https://docs.openshift.com/rosa/rosa_architecture/rosa_architecture_sub/rosa-basic-architecture-concepts.html#rosa-kubernetes-concept_rosa-basic-architecture-concepts\n",
    "    \"How can I federate metrics?\",    # https://mobb.ninja/docs/rosa/federated-metrics/\n",
    "    \"Is there any tool to help me troubleshoot my VPC connection?\", # https://docs.openshift.com/rosa/rosa_cluster_admin/cloud_infrastructure_access/dedicated-aws-peering.html#dedicated-aws-vpc-verifying-troubleshooting\n",
    "    \"What time is it?\",  # adversarial\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3288b31a-ac6f-4b1c-b232-25e31f28bfe5",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22553e66-e182-4f00-bc92-59fc0c07dc3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import boto3\n",
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from glob import glob\n",
    "from haystack import Document\n",
    "from haystack.document_stores import InMemoryDocumentStore, FAISSDocumentStore\n",
    "from haystack.nodes import BM25Retriever, FARMReader, RAGenerator, DensePassageRetriever, Seq2SeqGenerator\n",
    "from haystack.pipelines import ExtractiveQAPipeline, GenerativeQAPipeline, DocumentSearchPipeline, Pipeline\n",
    "from haystack.pipelines.standard_pipelines import TextIndexingPipeline\n",
    "from haystack.nodes import OpenAIAnswerGenerator\n",
    "from haystack.nodes import MarkdownConverter, PreProcessor\n",
    "from haystack.utils import convert_files_to_docs, print_answers, print_documents\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832cdea7-5df3-4df4-bc3a-12e2ef894e6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Input files\n",
    "\n",
    "We use a dataset consisting of a series of Markdown files about ROSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63398cc9-d1a9-4f7d-bffe-2d8f6d1b5633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading rosa-docs/adding_service_cluster.md\n",
      "Downloading rosa-docs/applications.md\n",
      "Downloading rosa-docs/authentication.md\n",
      "Downloading rosa-docs/cicd.md\n",
      "Downloading rosa-docs/logging.md\n",
      "Downloading rosa-docs/networking.md\n",
      "Downloading rosa-docs/ocm.md\n",
      "Downloading rosa-docs/rosa_architecture.md\n",
      "Downloading rosa-docs/rosa_backing_up_and_restoring_applications.md\n",
      "Downloading rosa-docs/rosa_cli.md\n",
      "Downloading rosa-docs/rosa_cluster_admin.md\n",
      "Downloading rosa-docs/rosa_getting_started.md\n",
      "Downloading rosa-docs/rosa_install_access_delete_clusters.md\n",
      "Downloading rosa-docs/rosa_planning.md\n",
      "Downloading rosa-docs/rosa_support.md\n",
      "Downloading rosa-docs/serverless.md\n",
      "Downloading rosa-docs/service_mesh.md\n",
      "Downloading rosa-docs/storage.md\n",
      "Downloading rosa-docs/upgrading.md\n",
      "Downloading rosa-docs/welcome.md\n"
     ]
    }
   ],
   "source": [
    "# Load custom environment variables\n",
    "## Create a .env file on your local with the correct configs\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")  # Required by the OpenAI generator\n",
    "\n",
    "# Obtain a copy of the current dataset\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                  endpoint_url=os.getenv(\"S3_ENDPOINT_URL\"),\n",
    "                  aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "                  aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "s3_bucket_name = os.getenv(\"S3_BUCKET_NAME\")\n",
    "s3_path = os.getenv(\"S3_PROJECT_KEY\", \"rosa-docs/\")  # Limit which files we get\n",
    "\n",
    "# Download data from Ceph\n",
    "data_dir = \"../data/raw\"\n",
    "for key in s3.list_objects(Bucket=s3_bucket_name)['Contents']:\n",
    "    if key[\"Key\"].startswith(s3_path):\n",
    "        print(\"Downloading\", key['Key'])\n",
    "        filename = os.path.join(data_dir, key[\"Key\"].removeprefix(s3_path))\n",
    "        s3.download_file(s3_bucket_name, key[\"Key\"], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b982cb-e75d-409f-bdb6-fdd3302773d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 74 files\n"
     ]
    }
   ],
   "source": [
    "# Where are the input text files\n",
    "# Besides the dataset, we also add the samples available in this repo\n",
    "local_samples = [\"../data/external/rosaworkshop\", \"../data/external/rh-mobb\"]\n",
    "doc_dirs = [data_dir] + local_samples\n",
    "\n",
    "# Which files will we consider\n",
    "file_pattern = \"*.md\"\n",
    "files_to_index = [file for doc_dir in doc_dirs for file in glob(os.path.join(doc_dir, file_pattern))]\n",
    "\n",
    "print(f\"There are {len(files_to_index)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1b6a07-4886-41d3-94d9-6562acba17bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extractive QA\n",
    "\n",
    "Extractive QA is about extracting an answer to the question from a given context. A context is provided so that the model can refer to it and make predictions about where the answer is inside the passage.\n",
    "\n",
    "This is a quick experiment based on the [Haystack simple tutorial for QA](https://haystack.deepset.ai/tutorials/01_basic_qa_pipeline). Like in the Haystack tutorial, here we are using a base RoBERTa model fine-tuned using the SQuAD 2.0 dataset, [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2)\n",
    "\n",
    "\n",
    "#### Pre-processing\n",
    "\n",
    "There is some pre-processing of the source files here, in order to ingest from Markdown and to accomodate the passage size to what the models can handle.\n",
    "\n",
    "As the purpose of this notebook is to explore what Haystack offers, we are using Haystack pre-processing tools.\n",
    "\n",
    "**NOTE**: splitting the documents here is most likely breaking the series of long steps that are present in several of the source files! In the future, this pre-processing should do proper markdown parsing and do e.g. meaningful splits that preserve the structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c47c488-e872-4d86-8c83-b2e28c60b551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pre-process docs\n",
    "# Ref: https://haystack.deepset.ai/tutorials/08_preprocessing#preprocessor\n",
    "# Quote: File splitting can have a very significant impact on the system’s performance and is absolutely mandatory for Dense Passage Retrieval models.\n",
    "\n",
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=False,\n",
    "    split_by=\"word\",\n",
    "    split_length=500,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")\n",
    "\n",
    "# Converting from markdown\n",
    "# Ref: https://docs.haystack.deepset.ai/docs/file_converters\n",
    "converter = MarkdownConverter(\n",
    "    remove_numeric_tables=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf91cee5-454c-4e9b-96f4-cee8f3712cb5",
   "metadata": {},
   "source": [
    "**FIXME**:\n",
    "I noticed a problem with the pre-processing above: it strips content it shouln't. Example:\n",
    "\n",
    "Original content (from rosa docs: rosa_planning.md):\n",
    "\n",
    "```\n",
    "The AWS IAM roles required to use OpenShift Cluster Manager are:\n",
    "\n",
    "-   `ocm-role`\n",
    "\n",
    "-   `user-role`\n",
    "\n",
    "Whether you manage your clusters using the `rosa` CLI or[...]\n",
    "```\n",
    "\n",
    "\n",
    "Content that ends up in the data store:\n",
    "\n",
    "```\n",
    "sqlite> select * from document where id=\"10a6cc97b3d4ede49529cb87db7b637f\";\n",
    "10a6cc97b3d4ede49529cb87db7b637f|2023-02-23 20:18:24|2023-02-23 20:18:24|\"The AWS IAM roles required to use OpenShift Cluster Manager are:\\n\\nWhether you manage your clusters using the   CLI or[...]\n",
    "```\n",
    "\n",
    "Notice how all the content within backquotes (like `rosa` or `user-role`) has been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a7dc27-a663-4f99-9570-78444d0d9cec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc9ac54be24443693611faedfb3f907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting files:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8b92d48a16461f8e61be8958c0fe29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/74 [00:00<?, ?docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.nodes.preprocessor.preprocessor:We found one or more sentences whose word count is higher than the split length.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9aab659ceae41d9a80abcff62a38329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating BM25 representation...:   0%|          | 0/705 [00:00<?, ? docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 705 documents from 74 files\n"
     ]
    }
   ],
   "source": [
    "# Initialize an in-memory document store\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "\n",
    "# Index the documents into the document store\n",
    "indexing_pipeline = TextIndexingPipeline(document_store=document_store, preprocessor=preprocessor, text_converter=converter)\n",
    "documents = indexing_pipeline.run_batch(file_paths=files_to_index)\n",
    "\n",
    "print(f\"There are {len(documents['documents'])} documents from {len(documents['file_paths'])} files\")\n",
    "\n",
    "# Initialize the retriever, reader, and retriever-reader pipeline\n",
    "retriever = BM25Retriever(document_store=document_store)\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=False)\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eefcf5-6c6f-40ec-8621-432960edd75d",
   "metadata": {},
   "source": [
    "#### Answering questions\n",
    "\n",
    "Here we are going to ask some questions and see the answers obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "032aa62f-cb6f-4e99-86f2-4a707a21dfde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.document_stores.memory:InMemoryDocumentStore does not support scale_score for BM25 retrieval. This parameter is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b946867fe094434b8485799d7d9c5d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.document_stores.memory:InMemoryDocumentStore does not support scale_score for BM25 retrieval. This parameter is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Do I have to sign a contract with Red Hat in order to deploy a ROSA cluster?\n",
      "Answers:\n",
      "[   {   'answer': 'You do not need to have a contract with Red Hat',\n",
      "        'context': 'Do I need to sign/have a contract with Red Hat?\\n'\n",
      "                   'No. You do not need to have a contract with Red Hat to use '\n",
      "                   'ROSA. You will need a Red Hat account for u'},\n",
      "    {   'answer': 'If this is the first time you are deploying ROSA in this '\n",
      "                  'account and have not yet created the account roles',\n",
      "        'context': 'sociated AWS account.\\n'\n",
      "                   'If this is the first time you are deploying ROSA in this '\n",
      "                   'account and have not yet created the account roles, then '\n",
      "                   'create the acc'},\n",
      "    {   'answer': 'You will need a Red Hat account',\n",
      "        'context': 'ou do not need to have a contract with Red Hat to use '\n",
      "                   'ROSA. You will need a Red Hat account for use on '\n",
      "                   'console.redhat.com which includes accepting our'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca693802061e4616bb3c0e129d1c4d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.document_stores.memory:InMemoryDocumentStore does not support scale_score for BM25 retrieval. This parameter is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Is ROSA GDPR Compliant?\n",
      "Answers:\n",
      "[   {   'answer': 'Yes:',\n",
      "        'context': 'r own backup policies for applications and data.\\n'\n",
      "                   'Is ROSA GDPR Compliant?\\n'\n",
      "                   'Yes: https://www.redhat.com/en/gdpr\\n'\n",
      "                   'Does the ROSA CLI accept Multi-region KMS'},\n",
      "    {   'answer': '2',\n",
      "        'context': 'rker nodes that a ROSA cluster can have?\\n'\n",
      "                   'For a ROSA cluster the minimum is 2 worker nodes for '\n",
      "                   'single AZ and 3 for multiple AZ.\\n'\n",
      "                   'Where can I find the pr'},\n",
      "    {   'answer': 'both methods are currently enabled',\n",
      "        'context': 'account in order to create and operate the cluster. While '\n",
      "                   'both methods are currently enabled, the “ROSA with STS” '\n",
      "                   'method is the preferred and recommen'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f36662b65824fe2a86517fb340f2029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.document_stores.memory:InMemoryDocumentStore does not support scale_score for BM25 retrieval. This parameter is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How can I upgrade my ROSA cluster?\n",
      "Answers:\n",
      "[   {   'answer': 'by using the   CLI',\n",
      "        'context': 'S cluster that uses the AWS Security Token Service (STS) '\n",
      "                   'manually by using the   CLI.\\n'\n",
      "                   'This method schedules the cluster for an immediate '\n",
      "                   'upgrade, if a'},\n",
      "    {   'answer': '180',\n",
      "        'context': 'r nodes that a cluster can support?\\n'\n",
      "                   'The maximum number of worker nodes is 180 per ROSA '\n",
      "                   'cluster.  See here for limits and scalability '\n",
      "                   'considerations an'},\n",
      "    {   'answer': '2',\n",
      "        'context': 'rker nodes that a ROSA cluster can have?\\n'\n",
      "                   'For a ROSA cluster the minimum is 2 worker nodes for '\n",
      "                   'single AZ and 3 for multiple AZ.\\n'\n",
      "                   'Where can I find the pr'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5c79f160be4af694d6ad4f49788099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.document_stores.memory:InMemoryDocumentStore does not support scale_score for BM25 retrieval. This parameter is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is STS?\n",
      "Answers:\n",
      "[   {   'answer': 'AWS Security Token Service',\n",
      "        'context': 'Red Hat OpenShift Service on AWS (ROSA) cluster that uses '\n",
      "                   'the AWS Security Token Service (STS).\\n'\n",
      "                   'Account\\n'\n",
      "                   '\\n'\n",
      "                   'You must ensure that the AWS limits are suffi'},\n",
      "    {   'answer': 'Security Token Service',\n",
      "        'context': 'ethod is the preferred and recommended option.\\n'\n",
      "                   'What is AWS STS (Security Token Service)?\\n'\n",
      "                   'As stated in the AWS documentation AWS STS “enables you to '\n",
      "                   're'},\n",
      "    {   'answer': 'AWS Security Token Service',\n",
      "        'context': ' into a customer’s existing Amazon Web Service (AWS) '\n",
      "                   'account.\\n'\n",
      "                   'AWS Security Token Service (STS) is the recommended '\n",
      "                   'credential mode for installing and i'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cdbddbe80749df8b3c1a59e0cf6852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.document_stores.memory:InMemoryDocumentStore does not support scale_score for BM25 retrieval. This parameter is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How is ROSA related to Kubernetes?\n",
      "Answers:\n",
      "[   {   'answer': 'affects all Kubernetes distributions',\n",
      "        'context': 'data store (etcd). This design is not unique to ROSA and '\n",
      "                   'affects all Kubernetes distributions. Anyone with API '\n",
      "                   'access can retrieve or modify a Secret,'},\n",
      "    {   'answer': '2',\n",
      "        'context': 'rker nodes that a ROSA cluster can have?\\n'\n",
      "                   'For a ROSA cluster the minimum is 2 worker nodes for '\n",
      "                   'single AZ and 3 for multiple AZ.\\n'\n",
      "                   'Where can I find the pr'},\n",
      "    {   'answer': 'you must first containerize your app by creating a '\n",
      "                  'container image that you store in a container registry',\n",
      "        'context': 'in Kubernetes on ROSA, you must first containerize your '\n",
      "                   'app by creating a container image that you store in a '\n",
      "                   'container registry.\\n'\n",
      "                   'Image\\n'\n",
      "                   'A container im'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbc1c93a63740e488fa715812513982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.document_stores.memory:InMemoryDocumentStore does not support scale_score for BM25 retrieval. This parameter is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How can I federate metrics?\n",
      "Answers:\n",
      "[   {   'answer': 'HTTP code',\n",
      "        'context': 'trics for Knative Eventing components.\\n'\n",
      "                   'By aggregating the metrics from HTTP code, events can be '\n",
      "                   'separated into two categories; successful events (2xx)'},\n",
      "    {   'answer': 'aggregating the metrics from HTTP code',\n",
      "        'context': 'e following metrics for Knative Eventing components.\\n'\n",
      "                   'By aggregating the metrics from HTTP code, events can be '\n",
      "                   'separated into two categories; successfu'},\n",
      "    {   'answer': 'using the MOBB Helm Chart to deploy the necessary agents to '\n",
      "                  'federate the metrics into AWS Prometheus',\n",
      "        'context': 'de will walk you through using the MOBB Helm Chart to '\n",
      "                   'deploy the necessary agents to federate the metrics into '\n",
      "                   'AWS Prometheus and then use Grafana to '}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d45742f43514234b57d9697122b739f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.document_stores.memory:InMemoryDocumentStore does not support scale_score for BM25 retrieval. This parameter is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Is there any tool to help me troubleshoot my VPC connection?\n",
      "Answers:\n",
      "[   {   'answer': 'verification procedure',\n",
      "        'context': 'k Save.\\n'\n",
      "                   '\\n'\n",
      "                   'The VPC peering connection is now complete. Follow the '\n",
      "                   'verification procedure to ensure connectivity across the '\n",
      "                   'peering connection is working'},\n",
      "    {   'answer': 'AWS documentation',\n",
      "        'context': ' public and private subnets and AWS Site-to-Site VPN '\n",
      "                   'access in the AWS documentation.\\n'\n",
      "                   '\\n'\n",
      "                   'Policies and service definition\\n'\n",
      "                   'About availability for Red Hat '},\n",
      "    {   'answer': 'Route Propagation',\n",
      "        'context': '\\n'\n",
      "                   '\\n'\n",
      "                   'After the VPN connection has been established, be sure to '\n",
      "                   'set up Route Propagation or the VPN may not function as '\n",
      "                   'expected.\\n'\n",
      "                   'Note the VPC subnet info'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9e6a41e3b94d2d98e31288d89fae93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What time is it?\n",
      "Answers:\n",
      "[   {   'answer': '2',\n",
      "        'context': 'rker nodes that a ROSA cluster can have?\\n'\n",
      "                   'For a ROSA cluster the minimum is 2 worker nodes for '\n",
      "                   'single AZ and 3 for multiple AZ.\\n'\n",
      "                   'Where can I find the pr'},\n",
      "    {   'answer': 'an hour',\n",
      "        'context': 'esources in your AWS account. After these credentials '\n",
      "                   'expire (typically an hour after being requested), they are '\n",
      "                   'no longer recognized by AWS and they '},\n",
      "    {   'answer': 'an hour after being requested',\n",
      "        'context': ' your AWS account. After these credentials expire '\n",
      "                   '(typically an hour after being requested), they are no '\n",
      "                   'longer recognized by AWS and they no longer h'}]\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    prediction = pipe.run(\n",
    "        query=question,\n",
    "        params={\n",
    "            \"Retriever\": {\"top_k\": 10},\n",
    "            \"Reader\": {\"top_k\": 3}\n",
    "        }\n",
    "    )\n",
    "    # Show a simplified list of answers\n",
    "    print_answers(\n",
    "        prediction,\n",
    "        details=\"minimum\" ## Choose from `minimum`, `medium`, and `all`\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6861e-bce6-4069-9a57-a14f9e9726fe",
   "metadata": {},
   "source": [
    "### Observations about extractive QA\n",
    "\n",
    "Some comments about the results:\n",
    "\n",
    "- All the answers are very short. Some questions would point to relatively long procedures, with multiple steps that are detailed in the documents. Other questions, however, do have short answers\n",
    "- Not all answers seem to make sense\n",
    "\n",
    "#### Next steps / TODO\n",
    "\n",
    "- Add metadata to the documents pointing to the original file, so that the question can eventually point to the location (URL) of the source\n",
    "- Proper testing and evaluation\n",
    "- Before that, however, it would be interesting to explore if there are ways to influence the lenght / extent of the answer; this is important for situations where the question is about a procedure that has clear steps to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6402981-3ff6-4bff-865c-b1b0354bba56",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generative QA\n",
    "\n",
    "Generative QA is about creating novel text during the answering process. While extractive QA highlights the span of text from the context that answers a query, generative QA creates new text.\n",
    "\n",
    "The general approach here is to start by obtaining vector embeddings to represent the domain-specific knowledge (context) and store them in a vector database. When time comes to generate an answer to a query, the process becomes:\n",
    "1. calculate the embedding for the question \n",
    "2. query the embeddings vector store to obtain a (set of) relevant documents\n",
    "3. **retrieve** the relevant documents (the ones found in the previous step) and pass them as context to the model, along with the question\n",
    "4. the model **generates** the answer\n",
    "\n",
    "The examples below use [Faiss](https://faiss.ai/) as the vector store.\n",
    "\n",
    "References:\n",
    "- https://docs.haystack.deepset.ai/docs/answer_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5556d00-e469-4837-a8f6-64101bf2aa5f",
   "metadata": {},
   "source": [
    "### LFQA\n",
    "\n",
    "Long-Form Question Answering (LFQA) is a variety of the generative question answering task. LFQA systems query large document stores for relevant information and then use this information to generate accurate, multi-sentence answers.\n",
    "\n",
    "In a extratcive question answering system, the retrieved documents related to the query (context passages) act directly as source tokens for extracted answers. In an LFQA system, context passages provide the context the system uses to generate original, abstractive, long-form answers.\n",
    "\n",
    "Here we use a Seq2seq generator with the [vblagoje/bart_lfqa model](https://huggingface.co/vblagoje/bart_lfqa) model.\n",
    "\n",
    "References:\n",
    "- https://haystack.deepset.ai/tutorials/12_lfqa\n",
    "- https://yjernite.github.io/lfqa.html\n",
    "- https://towardsdatascience.com/long-form-qa-beyond-eli5-an-updated-dataset-and-approach-319cb841aabb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a296c0a7-4194-4f40-aa5a-9a8b5aaff454",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a74618a47e45aa8e4a60f8d8801113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing Documents:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "faiss_index = \"faiss_lfqa.index\"\n",
    "faiss_config = \"faiss_lfqa.cfg\"\n",
    "\n",
    "if os.path.isfile(faiss_config):\n",
    "    document_store = FAISSDocumentStore.load(index_path=faiss_index, config_path=faiss_config)\n",
    "    # Delete existing documents in documents store\n",
    "    document_store.delete_documents()\n",
    "else:\n",
    "    document_store = FAISSDocumentStore(sql_url=\"sqlite:///faiss_rag.db\", embedding_dim=128)\n",
    "\n",
    "# Write documents to document store\n",
    "document_store.write_documents(documents[\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48feb956-c923-4e96-87ca-1f88fa39099e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa7a3a9b0bc49178a6a432e6c877eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating Embedding:   0%|          | 0/705 [00:00<?, ? docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create embeddings:   0%|          | 0/720 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"vblagoje/dpr-question_encoder-single-lfqa-wiki\",\n",
    "    passage_embedding_model=\"vblagoje/dpr-ctx_encoder-single-lfqa-wiki\",\n",
    ")\n",
    "\n",
    "document_store.update_embeddings(retriever)\n",
    "document_store.save(faiss_index)\n",
    "\n",
    "generator = Seq2SeqGenerator(model_name_or_path=\"vblagoje/bart_lfqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b014807-f102-40f7-b9e3-3fc298c63c69",
   "metadata": {},
   "source": [
    "Before jumping into question answering, let's do a quick check of the retriever only.\n",
    "\n",
    "The retriver allows us to select a set of documents that are most relevant to the question being asked, based on embedding similarity between the (embeddings of the) question and the contents of the vector store.\n",
    "\n",
    "Here we query the retriever using a question (query) and observe which passages (documents) it selects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "986c14cc-f9c3-4f5f-95ab-47fb2490ee57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Are AWS IAM roles relevant to ROSA?\n",
      "\n",
      "{   'content': 'Understanding ROSA\\n'\n",
      "               'Learn about Red Hat OpenShift Service on AWS (ROSA), '\n",
      "               'interacting with ROSA using Red Hat OpenShift Cluster Manager '\n",
      "               'and command-line interface (CLI) tools, consumption '\n",
      "               'experience, and integration with Amazon Web Services (AWS) '\n",
      "               'services.\\n'\n",
      "               'About ROSA\\n'\n",
      "               'ROSA is a fully-managed, turnkey application platform that '\n",
      "               'allows you to focus on delivering value to your customers by '\n",
      "               'building and deploying applications. Red Hat and AWS Site '\n",
      "               'reliability engineering (SRE) experts manage the underlying '\n",
      "               'platform...',\n",
      "    'name': None}\n",
      "\n",
      "{   'content': 'AWS Shared VPCs are not currently supported for ROSA '\n",
      "               'installations.\\n'\n",
      "               '\\n'\n",
      "               'You have completed the AWS prerequisites for ROSA with STS.\\n'\n",
      "               '\\n'\n",
      "               'You have available AWS service quotas.\\n'\n",
      "               '\\n'\n",
      "               'You have enabled the ROSA service in the AWS Console.\\n'\n",
      "               '\\n'\n",
      "               'You have installed and configured the latest ROSA CLI ( ) on '\n",
      "               'your installation host.\\n'\n",
      "               'To successfully install ROSA clusters, use the latest version '\n",
      "               'of the ROSA CLI.\\n'\n",
      "               '\\n'\n",
      "               'You have verified that the AWS Elastic Load Balancing (ELB) '\n",
      "               'service role exists in your AWS account.\\n'\n",
      "               '\\n'\n",
      "               'If you are configuri...',\n",
      "    'name': None}\n",
      "\n",
      "{   'content': 'If you are using AWS Organizations and you need to have a '\n",
      "               'service control policy (SCP) applied to the AWS account you '\n",
      "               'plan to use, these policies must not be more restrictive than '\n",
      "               'the roles and policies required by the cluster.\\n'\n",
      "               '\\n'\n",
      "               'Enable the ROSA service in the AWS Management Console.\\n'\n",
      "               '\\n'\n",
      "               'Sign in to your AWS account.\\n'\n",
      "               '\\n'\n",
      "               'To enable ROSA, go to the ROSA service and select Enable '\n",
      "               'OpenShift.\\n'\n",
      "               '\\n'\n",
      "               'Install and configure the AWS CLI.\\n'\n",
      "               '\\n'\n",
      "               'Follow the AWS command-line interface documentation to install '\n",
      "               'and configure the AWS CLI for...',\n",
      "    'name': None}\n",
      "\n",
      "{   'content': 'Before you create a ROSA cluster, you must enable ROSA in your '\n",
      "               'AWS account, install and configure the AWS CLI ( ) tool, and '\n",
      "               'verify the AWS CLI tool configuration.\\n'\n",
      "               '\\n'\n",
      "               'Install the ROSA and OpenShift CLI tools and verify the AWS '\n",
      "               'servce quotas. Install and configure the ROSA CLI ( ) and the '\n",
      "               'OpenShift CLI ( ). You can verify if the required AWS resource '\n",
      "               'quotas are available by using the ROSA CLI.\\n'\n",
      "               '\\n'\n",
      "               'Create a ROSA cluster or Create a ROSA cluster using AWS '\n",
      "               'PrivateLink. Use the ROSA CLI ( ) to create a cluster. You '\n",
      "               'ca...',\n",
      "    'name': None}\n",
      "\n",
      "{   'content': '\\n'\n",
      "               'Tyler Stacey\\n'\n",
      "               'Last updated 4 Oct 2022\\n'\n",
      "               'To proceed with the deployment of a ROSA cluster, an account '\n",
      "               'must support the required roles and permissions. AWS Service '\n",
      "               'Control Policies (SCPs) cannot block the API calls made by the '\n",
      "               'installer or operator roles.\\n'\n",
      "               'Details about the IAM resources required for an STS-enabled '\n",
      "               'installation of ROSA can be found here: '\n",
      "               'https://docs.openshift.com/rosa/rosa_architecture/rosa-sts-about-iam-resources.html\\n'\n",
      "               'This guide is validated for ROSA v4.11.X.\\n'\n",
      "               'Prerequisites\\n'\n",
      "               '\\n'\n",
      "               'AWS CLI\\n'\n",
      "               'ROSA CLI v1...',\n",
      "    'name': None}\n",
      "\n",
      "{   'content': 'The service enables cluster components to make AWS API calls '\n",
      "               'using secure cloud resource management practices.\\n'\n",
      "               'You can follow the workflow stages outlined in this section to '\n",
      "               'set up and access a ROSA cluster that uses STS.\\n'\n",
      "               '\\n'\n",
      "               'Complete the AWS prerequisites for ROSA with STS. To deploy a '\n",
      "               'ROSA cluster with STS, your AWS account must meet the '\n",
      "               'prerequisite requirements.\\n'\n",
      "               '\\n'\n",
      "               'Review the required AWS service quotas. To prepare for your '\n",
      "               'cluster deployment, review the AWS service quotas that are '\n",
      "               'required to run a ROSA clu...',\n",
      "    'name': None}\n",
      "\n",
      "{   'content': 'These credentials are associated to the IAM roles that are '\n",
      "               'specific to each component and cluster that makes AWS API '\n",
      "               'calls. This better aligns with principles of least-privilege '\n",
      "               'and is much better aligned to secure practices in cloud '\n",
      "               'service resource management. The ROSA CLI tool manages the STS '\n",
      "               'roles and policies that are assigned for unique tasks and '\n",
      "               'takes action upon AWS resources as part of OpenShift '\n",
      "               'functionality. One drawback of using STS is that roles and '\n",
      "               'policies must be created for each ROSA cluste...',\n",
      "    'name': None}\n",
      "\n",
      "{   'content': 'If you are using AWS Organizations, you can use an AWS account '\n",
      "               'within your organization or create a new one.\\n'\n",
      "               '\\n'\n",
      "               'Sign in to the AWS Management Console.\\n'\n",
      "               '\\n'\n",
      "               'Activate ROSA in your AWS account by navigating to the ROSA '\n",
      "               'service and selecting Enable OpenShift.\\n'\n",
      "               '\\n'\n",
      "               'Installing and configuring the required CLI tools\\n'\n",
      "               'Use the following steps to install and configure AWS, Red Hat '\n",
      "               'OpenShift Service on AWS (ROSA), and OpenShift CLI tools on '\n",
      "               'your workstation.\\n'\n",
      "               '\\n'\n",
      "               'You have an AWS account.\\n'\n",
      "               '\\n'\n",
      "               'You created a Red Hat account.\\n'\n",
      "               'You can creat...',\n",
      "    'name': None}\n",
      "\n",
      "{   'content': 'Red Hat Openshift for AWS (ROSA) comes with two built-in '\n",
      "               'monitoring stacks.   and  . They are both based on Prometheus, '\n",
      "               'the first targets the Cluster Operator (Red Hat SRE) and the '\n",
      "               'latter targets the Cluster user (you!).\\n'\n",
      "               \"Both provide amazing metrics insights inside the Cluster's web \"\n",
      "               'console, showing overall cluster metrics as well as namespace '\n",
      "               'specific workload metrics, all integrated with your configured '\n",
      "               'IDP.\\n'\n",
      "               'However the Alert Manager instance is locked down and used to '\n",
      "               'send alerts to the Red Hat  SRE team...',\n",
      "    'name': None}\n",
      "\n",
      "{   'content': 'About IAM resources for ROSA clusters that use STS\\n'\n",
      "               'To deploy a Red Hat OpenShift Service on AWS (ROSA) cluster '\n",
      "               'that uses the AWS Security Token Service (STS), you must '\n",
      "               'create the following AWS Identity Access Management (IAM) '\n",
      "               'resources:\\n'\n",
      "               '\\n'\n",
      "               'Specific account-wide IAM roles and policies that provide the '\n",
      "               'STS permissions required for ROSA support, installation, '\n",
      "               'control plane, and compute functionality. This includes '\n",
      "               'account-wide Operator policies.\\n'\n",
      "               '\\n'\n",
      "               'Cluster-specific Operator IAM roles that permit the ROSA '\n",
      "               'cluster O...',\n",
      "    'name': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_retrieval = DocumentSearchPipeline(retriever)\n",
    "res = p_retrieval.run(query=\"Are AWS IAM roles relevant to ROSA?\", params={\"Retriever\": {\"top_k\": 10}})\n",
    "print_documents(res, max_text_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8b449-c5c5-4d30-9f99-b57adc1901a9",
   "metadata": {},
   "source": [
    "#### Question answering\n",
    "\n",
    "Now let's answer the same set of questions as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "296561a5-1a02-4301-b056-89d0e3e17628",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Do I have to sign a contract with Red Hat in order to deploy a ROSA cluster?\n",
      "Answers:\n",
      "[   {   'answer': 'Yes, you have to sign a contract with Red Hat in order to '\n",
      "                  'deploy a ROSA cluster.'}]\n",
      "\n",
      "Query: Is ROSA GDPR Compliant?\n",
      "Answers:\n",
      "[   {   'answer': 'ROSA is not GDPR compliant. It is a cloud service, which '\n",
      "                  'means that it is not governed by the GDPR. However, it is '\n",
      "                  'subject to the same laws as any other cloud service.'}]\n",
      "\n",
      "Query: How can I upgrade my ROSA cluster?\n",
      "Answers:\n",
      "[   {   'answer': 'ROSA with STS requires you to install the latest version of '\n",
      "                  \"Red Hat OpenShift. If you don't want to do that, you can \"\n",
      "                  'upgrade your cluster to a newer version of OpenShift using '\n",
      "                  'the AWS IAM Console.'}]\n",
      "\n",
      "Query: What is STS?\n",
      "Answers:\n",
      "[   {   'answer': 'ROSA is a service that needs to manage infrastructure '\n",
      "                  'resources in your AWS account. In order to manage these '\n",
      "                  'resources, you need to have access to them. The easiest way '\n",
      "                  'to do this is to create a \"role\" that you can assign to the '\n",
      "                  \"resources you want to manage. For example, let's say you \"\n",
      "                  'have a server in your house. You want to use that server to '\n",
      "                  \"do some work, but you don't want anyone else to know what \"\n",
      "                  \"you're doing. Instead, you create a role for that server, \"\n",
      "                  'and you give it the ability to do whatever you want with '\n",
      "                  'that server. In this case, you give the server your \"role\", '\n",
      "                  'and the server can do whatever it wants with that role. The '\n",
      "                  \"problem with this is that the server doesn't know what your \"\n",
      "                  \"role is, so it can't do anything with it. The solution is \"\n",
      "                  'to use AWS Security Token Service (STS), which allows you '\n",
      "                  'to grant the server a temporary, limited-privilege access'}]\n",
      "\n",
      "Query: How is ROSA related to Kubernetes?\n",
      "Answers:\n",
      "[   {   'answer': \"Kubernetes is a container orchestration system. It's \"\n",
      "                  'basically a set of tools that allow you to create and '\n",
      "                  'manage a cluster of containers. ROSA is a tool that allows '\n",
      "                  'you to automate the deployment of a Kubernetes cluster.'}]\n",
      "\n",
      "Query: How can I federate metrics?\n",
      "Answers:\n",
      "[   {   'answer': \"You can't federate metrics, but you can use the following \"\n",
      "                  'metrics to debug and see how they are performing by the '\n",
      "                  'channels. You can also measure the latency of the filtering '\n",
      "                  'action on an event. The time taken to dispatch an event to '\n",
      "                  'a channel. The number of events received by a broker. Time '\n",
      "                  'taken to process an event before it is dispatched to a '\n",
      "                  'trigger subscriber.'}]\n",
      "\n",
      "Query: Is there any tool to help me troubleshoot my VPC connection?\n",
      "Answers:\n",
      "[   {   'answer': \"I'm not sure if this is what you're looking for, but you \"\n",
      "                  'might want to try /r/explainlikeimfive.'}]\n",
      "\n",
      "Query: What time is it?\n",
      "Answers:\n",
      "[   {   'answer': 'It depends on what you mean by \"time\". If you mean the time '\n",
      "                  'it takes for a server to respond to a request, then yes, '\n",
      "                  \"it's time. If you're talking about the amount of time it \"\n",
      "                  'would take for a request to be processed, then it depends '\n",
      "                  'on how fast the server is responding to the request. For '\n",
      "                  'example, if you want to know how long it takes to process a '\n",
      "                  'request from a web server, then you need to know the time '\n",
      "                  'that it takes the server to process the request from the '\n",
      "                  'web server.'}]\n"
     ]
    }
   ],
   "source": [
    "pipe = GenerativeQAPipeline(generator=generator, retriever=retriever)\n",
    "for question in questions:\n",
    "    res = pipe.run(query=question, params={\"Retriever\": {\"top_k\": 3}})\n",
    "    print_answers(res, details=\"medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b43f09-9ce1-4d36-9b27-30ed2bb698ee",
   "metadata": {},
   "source": [
    "### OpenAI\n",
    "\n",
    "The OpenAI Answer generator uses OpenAI's completion API to generate answers.\n",
    "\n",
    "The pipeline used here is similar to the previous one in that it uses the same dense passage retrieval approach to prepare context that is submitted to the completion API together with the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc692f74-624e-4739-a648-f86d509a9970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's initiate the OpenAIAnswerGenerator \n",
    "generator = OpenAIAnswerGenerator(\n",
    "    api_key=api_key,\n",
    "    model=\"text-davinci-003\",\n",
    "    max_tokens=150,\n",
    "    presence_penalty=0.1,\n",
    "    frequency_penalty=0.1,\n",
    "    top_k=2,\n",
    "    temperature=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4bfc016-2d71-4aa9-8877-58dcbb8ab3a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = GenerativeQAPipeline(generator=generator, retriever=retriever) # Same retriever as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62bea0a5-1f8c-4a1b-b38e-a0fa82dc39b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Do I have to sign a contract with Red Hat in order to deploy a ROSA cluster?\n",
      "Answers:\n",
      "[   {   'answer': ' No, you do not have to sign a contract with Red Hat in '\n",
      "                  'order to deploy a ROSA cluster. However, you must meet the '\n",
      "                  'customer requirements listed in the documentation and '\n",
      "                  'ensure that your AWS account is set up according to the Red '\n",
      "                  'Hat prerequisites.'},\n",
      "    {   'answer': ' No, you do not have to sign a contract with Red Hat in '\n",
      "                  'order to deploy a ROSA cluster.'}]\n",
      "\n",
      "Query: Is ROSA GDPR Compliant?\n",
      "Answers:\n",
      "[   {   'answer': ' ROSA does not have GDPR compliance built into the '\n",
      "                  'platform. However, users may be able to configure their own '\n",
      "                  'solutions to achieve GDPR compliance within their '\n",
      "                  'environment.'},\n",
      "    {   'answer': ' ROSA does not have GDPR compliance features built into the '\n",
      "                  'platform. However, customers can take steps to ensure their '\n",
      "                  'deployment of ROSA is GDPR compliant.'}]\n",
      "\n",
      "Query: How can I upgrade my ROSA cluster?\n",
      "Answers:\n",
      "[   {   'answer': ' You can upgrade your ROSA cluster through the OpenShift '\n",
      "                  'Cluster Manager console. Log in to OpenShift Cluster '\n",
      "                  'Manager Hybrid Cloud Console, select a cluster to upgrade, '\n",
      "                  'and click the Settings tab. In the Update strategy pane, '\n",
      "                  'select Individual Updates or Recurring Updates and follow '\n",
      "                  'the onscreen instructions to upgrade the cluster.'},\n",
      "    {   'answer': ' You can upgrade your ROSA cluster by scheduling individual '\n",
      "                  'upgrades through the OpenShift Cluster Manager console or '\n",
      "                  'scheduling recurring upgrades for your cluster.'}]\n",
      "\n",
      "Query: What is STS?\n",
      "Answers:\n",
      "[   {   'answer': ' AWS Security Token Service (STS) enables you to request '\n",
      "                  'temporary, limited-privilege credentials for AWS Identity '\n",
      "                  'and Access Management (IAM) users or for users you '\n",
      "                  'authenticate (federated users).'},\n",
      "    {   'answer': ' AWS Security Token Service (STS) is a service that enables '\n",
      "                  'you to request temporary, limited-privilege credentials for '\n",
      "                  'AWS Identity and Access Management (IAM) users or for users '\n",
      "                  'you authenticate (federated users).'}]\n",
      "\n",
      "Query: How is ROSA related to Kubernetes?\n",
      "Answers:\n",
      "[   {   'answer': ' ROSA can be used to create clusters on Kubernetes, '\n",
      "                  'configure identity providers, grant cluster administrator '\n",
      "                  'privileges to identity provider users, quickly access a '\n",
      "                  'newly deployed cluster, revoke access to a cluster for a '\n",
      "                  'user, delete a cluster, and check service quotas.'},\n",
      "    {   'answer': ' ROSA makes use of Kubernetes by allowing users to '\n",
      "                  'configure identity providers, grant cluster administrator '\n",
      "                  'privileges to identity provider users, access newly '\n",
      "                  'deployed clusters quickly by configuring a user, revoke '\n",
      "                  'access to a cluster from a user, and delete a cluster.'}]\n",
      "\n",
      "Query: How can I federate metrics?\n",
      "Answers:\n",
      "[   {   'answer': ' You can use the `spec.monitoring.serviceMonitor.federate` '\n",
      "                  'property for a `ServiceMonitor` resource to specify the TLS '\n",
      "                  'configuration to use when scraping metrics from an '\n",
      "                  'endpoint. The `federate` property is not yet available for '\n",
      "                  '`PodMonitor` resources. If you need to use a TLS '\n",
      "                  'configuration when scraping metrics, you must use the '\n",
      "                  '`ServiceMonitor` resource.'},\n",
      "    {   'answer': ' You can use the  property for a  resource to specify the '\n",
      "                  'TLS configuration to use when scraping metrics from an '\n",
      "                  'endpoint. The  property is not yet available for  '\n",
      "                  'resources. If you need to use a TLS configuration when '\n",
      "                  'scraping metrics, you must use  the  resource.'}]\n",
      "\n",
      "Query: Is there any tool to help me troubleshoot my VPC connection?\n",
      "Answers:\n",
      "[   {   'answer': ' Yes, the AWS Knowledge Center has tools to help '\n",
      "                  'troubleshoot VPC connections. Additionally, setting up an '\n",
      "                  'SLA Monitor (Cisco ASA) or another device on your side of '\n",
      "                  'the tunnel to constantly send \"interesting\" traffic can '\n",
      "                  'help keep the tunnel connection Up consistently.'},\n",
      "    {   'answer': ' Yes, the AWS Knowledge Center provides several tools to '\n",
      "                  'help troubleshoot VPC connections, such as ensuring that '\n",
      "                  'the source traffic is coming from the same IP as the '\n",
      "                  'configured customer gateway, checking route tables, and '\n",
      "                  'confirming there are no firewall rules that could be '\n",
      "                  'causing an interruption. Additionally, Red Hat recommends '\n",
      "                  'setting up an SLA Monitor (Cisco ASA) or similar device to '\n",
      "                  'constantly send \"interesting\" traffic.'}]\n",
      "\n",
      "Query: What time is it?\n",
      "Answers:\n",
      "[   {'answer': ' This question is unrelated to the given context.'},\n",
      "    {'answer': ' This question is not related to the context provided.'}]\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    res = pipe.run(query=question, params={\"Retriever\": {\"top_k\": 2}})\n",
    "    print_answers(res, details=\"medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61949f6-f329-41dc-84d1-f6ab9b4fcda2",
   "metadata": {},
   "source": [
    "#### Observations about OpenAI based answers\n",
    "\n",
    "- The generated text looks more elaborate / friendly than other alternatives\n",
    "- The adversarial example is correctly handled, unlike in other alternatives\n",
    "- Still, some answers are just wrong (e.g. GDPR)\n",
    "\n",
    "## Next steps / TODO\n",
    "\n",
    "- FIXME: proper markdown parsing that:\n",
    "  - does not remove content\n",
    "  - splits documents while preserving meaningful sturcture\n",
    "- Identify the context (i.e. which sources were used to obtain the answers) in all the answers. Include links to sources\n",
    "- It would be interesting to be able to observe the API calls that the pipeline does, i.e. which prompts are being used"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
