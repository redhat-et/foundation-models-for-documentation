{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Footprint\n",
    "\n",
    "This notebook computes gpu requirements of models with different number of model parameters. The results are shown in the outputs.\n",
    "\n",
    "Source: https://github.com/hwchase17/langchain/blob/master/langchain/llms/huggingface_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be76f7630cb8475eae71a1d99138ddcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "import GPUtil\n",
    "from numba import cuda\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | Name     | Serial        | UUID                                     || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  0 | Tesla T4 | 1323921034295 | GPU-634f99e4-90c8-d992-a802-6d4246152a1c ||       22C |        0% |           0% ||      15360MB |         5MB |     14923MB || Enabled      | Disabled       |\n"
     ]
    }
   ],
   "source": [
    "# View GPU utilization when nothing is running on it\n",
    "device = -1  # cpu\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "    device = 0  # first GPU\n",
    "# Get all available GPUs\n",
    "gpus = GPUtil.getGPUs()\n",
    "GPUtil.showUtilization(all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | Name     | Serial        | UUID                                     || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  0 | Tesla T4 | 1323921034295 | GPU-634f99e4-90c8-d992-a802-6d4246152a1c ||       23C |        6% |           3% ||      15360MB |       481MB |     14447MB || Enabled      | Disabled       |\n"
     ]
    }
   ],
   "source": [
    "# Look at the GPU utilization when kernels are loaded\n",
    "torch.ones((1, 1)).to(device)\n",
    "GPUtil.showUtilization(all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a memory of 481MB used just for loading kernels. Let's see what happens when the models are loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model selection\n",
    "# A dictionary of models and their corresponding tasks\n",
    "models = {\n",
    "    \"bloom-1b7\": {\n",
    "        \"task\": \"text-generation\",\n",
    "        \"model\": \"bigscience/bloom-1b7\",\n",
    "        \"extra_args\": {\"max_new_tokens\": 100},\n",
    "    },\n",
    "    \"bloomz-1b7\": {\n",
    "        \"task\": \"text-generation\",\n",
    "        \"model\": \"bigscience/bloomz-1b7\",\n",
    "        \"extra_args\": {\"max_new_tokens\": 500, \"temperature\": 0.9},\n",
    "    },\n",
    "    \"gpt2\": {\n",
    "        \"task\": \"text-generation\",\n",
    "        \"model\": \"gpt2\",\n",
    "        \"extra_args\": {\"max_new_tokens\": 30},\n",
    "    },\n",
    "    \"gpt2-large\": {\n",
    "        \"task\": \"text-generation\",\n",
    "        \"model\": \"gpt2-large\",\n",
    "        \"extra_args\": {\"max_new_tokens\": 50},\n",
    "    },\n",
    "    \"mt0-large\": {\n",
    "        \"task\": \"text2text-generation\",\n",
    "        \"model\": \"bigscience/mt0-large\",\n",
    "        \"extra_args\": {},\n",
    "    },\n",
    "    \"opt-1b3\": {\n",
    "        \"task\": \"text-generation\",\n",
    "        \"model\": \"facebook/opt-1.3b\",\n",
    "        \"extra_args\": {\"max_new_tokens\": 100},\n",
    "    },\n",
    "    \"bloom-3b\": {\n",
    "        \"task\": \"text-generation\",\n",
    "        \"model\": \"bigscience/bloom-3b\",\n",
    "        \"extra_args\": {\"max_new_tokens\": 100},\n",
    "    },\n",
    "    \"bloom-7b1\": {\n",
    "        \"task\": \"text-generation\",\n",
    "        \"model\": \"bigscience/bloom-7b1\",\n",
    "        \"extra_args\": {\"max_new_tokens\": 100},\n",
    "    },\n",
    "    \"gpt-6b\":{\n",
    "        \"task\": \"text-generation\",\n",
    "        \"model\": \"EleutherAI/gpt-j-6B\",\n",
    "        \"extra_args\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_on_gpu(model_id):\n",
    "    \"\"\"\n",
    "    The function loads the model (from its model_id) on the available GPU\n",
    "    \"\"\"\n",
    "    # A mapping between the task and the corresponding transformers class\n",
    "    auto_classes = {\n",
    "        \"text-generation\": AutoModelForCausalLM,\n",
    "        \"text2text-generation\": AutoModelForSeq2SeqLM,\n",
    "    }\n",
    "\n",
    "    task = models[model_id][\"task\"]\n",
    "    model_name = models[model_id][\"model\"]\n",
    "    auto_class = auto_classes[task]\n",
    "    print(f\"Using a {task} pipeline with {model_name}\")\n",
    "\n",
    "    model = auto_class.from_pretrained(model_name).to(device)\n",
    "\n",
    "def show_utilization_models(model_ids):\n",
    "    \"\"\"\n",
    "    This function loads all the models from their model_ids, shows their gpu utilization and cleans up memory.\n",
    "    \"\"\"\n",
    "    for model_id in model_ids:\n",
    "        load_model_on_gpu(model_id)\n",
    "        GPUtil.showUtilization(all=True)\n",
    "        torch.cuda.empty_cache()\n",
    "        cuda.get_current_device().reset()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a text-generation pipeline with bigscience/bloom-1b7\n",
      "| ID | Name     | Serial        | UUID                                     || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  0 | Tesla T4 | 1320220104182 | GPU-e038c2e6-e540-d3ae-f932-68e51a562fd3 ||       33C |       20% |          59% ||      15360MB |      9011MB |      5917MB || Enabled      | Disabled       |\n",
      "Using a text-generation pipeline with bigscience/bloomz-1b7\n",
      "| ID | Name     | Serial        | UUID                                     || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  0 | Tesla T4 | 1320220104182 | GPU-e038c2e6-e540-d3ae-f932-68e51a562fd3 ||       32C |       11% |          59% ||      15360MB |      9011MB |      5917MB || Enabled      | Disabled       |\n",
      "Using a text-generation pipeline with gpt2-large\n",
      "| ID | Name     | Serial        | UUID                                     || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  0 | Tesla T4 | 1320220104182 | GPU-e038c2e6-e540-d3ae-f932-68e51a562fd3 ||       32C |       25% |          25% ||      15360MB |      3863MB |     11065MB || Enabled      | Disabled       |\n",
      "Using a text-generation pipeline with facebook/opt-1.3b\n",
      "| ID | Name     | Serial        | UUID                                     || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  0 | Tesla T4 | 1320220104182 | GPU-e038c2e6-e540-d3ae-f932-68e51a562fd3 ||       31C |        4% |          38% ||      15360MB |      5897MB |      9031MB || Enabled      | Disabled       |\n",
      "Using a text2text-generation pipeline with bigscience/mt0-large\n",
      "| ID | Name     | Serial        | UUID                                     || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  0 | Tesla T4 | 1320220104182 | GPU-e038c2e6-e540-d3ae-f932-68e51a562fd3 ||       32C |        5% |          35% ||      15360MB |      5317MB |      9611MB || Enabled      | Disabled       |\n"
     ]
    }
   ],
   "source": [
    "model_ids_1 = [\"bloom-1b7\", \"bloomz-1b7\", \"gpt2-large\", \"opt-1b3\", \"mt0-large\"]\n",
    "show_utilization_models(model_ids_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a text-generation pipeline with bigscience/bloom-3b\n",
      "| ID | Name     | Serial        | UUID                                     || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  0 | Tesla T4 | 1320220104182 | GPU-e038c2e6-e540-d3ae-f932-68e51a562fd3 ||       35C |       17% |          94% ||      15360MB |     14443MB |       485MB || Enabled      | Disabled       |\n"
     ]
    }
   ],
   "source": [
    "model_ids_2 = [\"bloom-3b\"]\n",
    "show_utilization_models(model_ids_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a text-generation pipeline with EleutherAI/gpt-j-6B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a28c62da494433189cd35e3928ecabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/930 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f642a8222cf3463fa1780f74485ac67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/24.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ids_3 = [\"gpt-6b\", \"bloom-7b1\"]\n",
    "show_utilization_models(model_ids_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3B model takes around 94% of the GPU RAM and therefore 6B and 7B models can't be loaded into the GPU in fp32 precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "This notebook loads models on the GPU and checks their GPU memory utilization. This cluster has Tesla T4 16GB GPUs out of which 15.36GB are available for use. The kernels take an additional 500MB. The results show memory taken up by different small models. The models that have no results were not able to run in the environment resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "dfef540ac477f3be91e5308acd671ac2a81f2c8cc1125947821b3502f71b441e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
